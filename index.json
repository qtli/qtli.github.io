[{"authors":["admin"],"categories":null,"content":"Hi! I\u0026rsquo;m a third-year PhD student in computer science at The University of Hong Kong, advised by Lingpeng Kong and BCM Kao. I have previously interned at Tencent AI Lab, working with Piji Li and Wei Bi.\nMy primary research interests lie in the area of natural language processing and machine learning. My research aims to build robust and explainable AI systems that can understand natural language text and solve real-world problems, concerning both performance and social implications. Toward this goal, my work spans LLM alignment, long-range planning, and applications in mathematical reasoning.\n","date":1733665699,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1733665699,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://qtli.github.io/author/qintong-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/qintong-li/","section":"authors","summary":"Hi! I\u0026rsquo;m a third-year PhD student in computer science at The University of Hong Kong, advised by Lingpeng Kong and BCM Kao. I have previously interned at Tencent AI Lab, working with Piji Li and Wei Bi.","tags":null,"title":"Qintong Li","type":"authors"},{"authors":["Qintong Li","Jiahui Gao","Sheng Wang","Renjie Pi","Xueliang Zhao","Chuan Wu","Xin Jiang","Zhenguo Li","Lingpeng Kong"],"categories":[],"content":"","date":1733665699,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733665699,"objectID":"9b5696912ecb0d5f5c1d213603db8934","permalink":"https://qtli.github.io/publication/reversegen/","publishdate":"2024-12-08T21:48:19+08:00","relpermalink":"/publication/reversegen/","section":"publication","summary":"We presents ReverseGen, a new paradigm for generating effective synthetic data from the “failure” cases of a target model on specific tasks. We optimize a language model by rewarding it for generating instructions that cause failures in the target model while employing a selection strategy to maintain instruction diversity. This optimization objective is achieved through an iterative preference learning algorithm.","tags":[],"title":"Forewarned is Forearmed: Leveraging LLMs for Data Synthesis through Failure-Inducing Exploration","type":"publication"},{"authors":["Qintong Li","Leyang Cui","Lingpeng Kong","Wei Bi"],"categories":[],"content":"","date":1731583002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1731583002,"objectID":"406d89cb8218553ae09b348491220539","permalink":"https://qtli.github.io/publication/coeval/","publishdate":"2024-11-14T19:16:42+08:00","relpermalink":"/publication/coeval/","section":"publication","summary":"Our analysis shows that 1) LLM evaluators can generate unnecessary criteria or omit crucial criteria, resulting in a slight deviation from the experts. 2) LLM evaluators excel in general criteria, such as fluency, but face challenges with complex criteria, such as numerical reasoning. (COLING 2025)","tags":["llm-as-judge","reliability","collaborative evaluation"],"title":"Exploring the Reliability of Large Language Models as Customized Evaluators for Diverse NLP Tasks","type":"publication"},{"authors":["Chang Ma","Haiteng Zhao","Lin Zheng","Jiayi Xin","Qintong Li","Lijun Wu","Zhihong Deng","Yang Lu","Qi Liu","Lingpeng Kong"],"categories":[],"content":"","date":1728714811,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728714811,"objectID":"00b9d38496eb9c2aa97ffd2f12c8af4b","permalink":"https://qtli.github.io/publication/protein/","publishdate":"2024-05-12T14:33:31+08:00","relpermalink":"/publication/protein/","section":"publication","summary":"We introduce Retrieved Sequence Augmentation(RSA) for protein representation learning without additional alignment or pre-processing. RSA links query protein sequences to a set of sequences with similar structures or properties in the database and combines these sequences for downstream prediction. (EMNLP 2024)","tags":[],"title":"Retrieved Sequence Augmentation for Protein Representation Learning","type":"publication"},{"authors":["Qintong Li","Leyang Cui","Xueliang Zhao","Lingpeng Kong","Wei Bi"],"categories":[],"content":"","date":1723357553,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723357553,"objectID":"7bad332e725a72b28848f48804c25e53","permalink":"https://qtli.github.io/publication/gsmplus/","publishdate":"2024-10-09T16:54:19+08:00","relpermalink":"/publication/gsmplus/","section":"publication","summary":"We introduce the adversarial grade school math (GSM-Plus) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. (ACL 2024)","tags":[],"title":"GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers","type":"publication"},{"authors":["Yafu Li","Qintong Li","Leyang Cui","Wei Bi","Longyue Wang","Linyi Yang","Shuming Shi","Yue Zhang"],"categories":[],"content":"","date":1723357553,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723357553,"objectID":"ceb44d1e27e66bdaaa5ff4e843a7b34b","permalink":"https://qtli.github.io/publication/detection/","publishdate":"2023-05-23T14:25:53+08:00","relpermalink":"/publication/detection/","section":"publication","summary":"Recent advances in large language models highlights the importance of deepfake text detection to avoid potential risks such as fake news propagation and plagiarism. We build a wild testbed by gathering texts from various human writings and deepfake texts generated by different LLMs. (ACL 2024)","tags":[],"title":"MAGE: Machine-generated Text Detection in the Wild","type":"publication"},{"authors":["Xueliang Zhao","Xinting Huang","Tingchen Fu","Qintong Li","Shansan Gong","Lemao Liu","Wei Bi","Lingpeng Kong"],"categories":[],"content":"","date":1723271153,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723271153,"objectID":"4290c5cc9db628c02578ced295611830","permalink":"https://qtli.github.io/publication/sego/","publishdate":"2024-10-09T16:46:20+08:00","relpermalink":"/publication/sego/","section":"publication","summary":"We introduce the Bi-Modal Behavioral Alignment prompting method, designed to maximize the potential of Domain-Specific Languages (DSL) in augmenting complex multi-modal reasoning tasks. BBA initiates by guiding large vision-language models to create separate reasoning chains for visual and DSL representations. (Findings of ACL 2024)","tags":[],"title":"BBA: Bi-Modal Behavioral Alignment for Reasoning with Large Vision-Language Models","type":"publication"},{"authors":null,"categories":null,"content":"I serve as a teacher assistant for course Natural Language Processing (for masters) COMP3361. (2023 Fall, 2024 Fall)\n","date":1693526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693526400,"objectID":"b5d09da959a1d29fd7c3d71c698e77c1","permalink":"https://qtli.github.io/post/ta_comp7603/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/post/ta_comp7603/","section":"post","summary":"I serve as a teacher assistant for course Natural Language Processing (for masters) COMP3361. (2023 Fall, 2024 Fall)","tags":null,"title":"TA of COMP7603","type":"post"},{"authors":["Qintong Li","Zhiyong Wu","Lingpeng Kong","Wei Bi"],"categories":[],"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682943621,"objectID":"218ed2f11984f83460da19e30f974061","permalink":"https://qtli.github.io/publication/eib/","publishdate":"2023-05-01T12:20:21.456496Z","relpermalink":"/publication/eib/","section":"publication","summary":"Explanations generated through single-pass prompting often lack sufficiency and conciseness, we develop an information bottleneck method to produce refined explanations that are sufficient and concise. (Findings of ACL 2023)","tags":["explanation generation","information bottleneck"],"title":"Explanation Regeneration via Information Bottleneck","type":"publication"},{"authors":["Jiyue Jiang","Sheng Wang","Qintong Li","Lingpeng Kong","Chuan Wu"],"categories":[],"content":"","date":1682726400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682770821,"objectID":"c7f2e7a9a291730bddd9c641733dc982","permalink":"https://qtli.github.io/publication/csconv/","publishdate":"2023-04-29T12:20:21.456496Z","relpermalink":"/publication/csconv/","section":"publication","summary":"When communicating with elders with cognitive impairment, cognitive stimulation (CS) helps to maintain the cognitive health of elders. We construct a Chinese CS conversation dataset and propose a multi-source knowledge fusion method for CS dialogue. (ACL 2023)","tags":["dialogue generation","cognitive stimulation"],"title":"A Cognitive Stimulation Dialogue System with Multi-source Knowledge Fusion for Elders with Cognitive Impairment","type":"publication"},{"authors":["Jiacheng Ye","Jiahui Gao","Qintong Li","Hang Xu","Jiangtao Feng","Zhiyong Wu","Tao Yu","Lingpeng Kong"],"categories":[],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638361221,"objectID":"1a2e4cd285441b7eed1668bdf12fd70b","permalink":"https://qtli.github.io/publication/zerogen/","publishdate":"2022-12-01T12:20:21.456496Z","relpermalink":"/publication/zerogen/","section":"publication","summary":"We study a flexible and efficient zero-short learning method. Given a zero-shot task, we first generate a dataset from scratch using PLMs in an unsupervised manner. Then, we train a tiny task model under the supervision of the synthesized dataset. (EMNLP 2022)","tags":["dataset generation","zero-shot learning"],"title":"Efficient Zero-shot Learning via Dataset Generation","type":"publication"},{"authors":["Qintong Li","Piji Li","Wei Bi","Zhaochun Ren","Yuxuan Lai","Lingpeng Kong"],"categories":[],"content":"","date":1653223235,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653223235,"objectID":"65fcb9db40892fa0b7ea631b918ac13b","permalink":"https://qtli.github.io/publication/eventplan4textgen/","publishdate":"2022-03-16T20:40:35+08:00","relpermalink":"/publication/eventplan4textgen/","section":"publication","summary":"A novel two-stage method which explicitly arranges the ensuing events in open-ended text generation. (Findings of ACL 2022)","tags":["open-ended text generation","event"],"title":"Event Transition Planning for Open-ended Text Generation","type":"publication"},{"authors":["Qintong Li","Piji Li","Zhaochun Ren","Pengjie Ren","Zhumin Chen"],"categories":[],"content":"","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638361221,"objectID":"f301b48b02e943dbd36dd8702d99c1b0","permalink":"https://qtli.github.io/publication/kemp/","publishdate":"2021-12-01T12:20:21.456496Z","relpermalink":"/publication/kemp/","section":"publication","summary":"The first attempt to leverage external knowledge to accurately perceive and appropriately express implicit emotions in empathetic dialogue generation. (AAAI 2022)","tags":["empathetic dialogue generation","knowledge"],"title":"Knowledge Bridging for Empathetic Dialogue Generation","type":"publication"},{"authors":null,"categories":null,"content":"I serve as a teacher assistant for course Natural Language Processing (for undergraduates) COMP3361. (2021 Fall, 2022 Fall)\n","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"fdcbc820d94364fbc5f7ccdeceae07db","permalink":"https://qtli.github.io/post/ta_comp3361/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/post/ta_comp3361/","section":"post","summary":"I serve as a teacher assistant for course Natural Language Processing (for undergraduates) COMP3361. (2021 Fall, 2022 Fall)","tags":null,"title":"TA of COMP3361","type":"post"},{"authors":null,"categories":null,"content":"Description eComTag is collected from Chinese e-commerce websites, containing reviews and opinion tags for 50,068 items. This dataset is to facilitate the abstractive opinion tagging task, which aims to generate opinion tags based on large volumes of item reviews.\nDisclaimer The eComTag dataset is available strictly for non-commercial research purposes only.\n  All reviews and tags are obtained from the Internet, which is not the property of the authors, or any associated employers, entities or institutions. We do not bear responsibility for either the content or meaning of these reviews and answers.\n  By requesting and/or using this dataset, you agree not to reproduce, duplicate, copy, sell, trade, resell, rent or exploit for any commercial purpose, any portion of the contexts and any portion of derived data.\n  We reserve the right to terminate your access to the eComTag dataset at any time.\n  Download Please use this Google form to submit your information and request access to eComTag.\nData Format Readers can directly refer to the README.md in the downloaded file for more extensive instructions. Also, we provide a brief overview in the github repository.\nThe whole dataset, including train, validation, and test, is saved in a pickle file (ecomtag_dataset_preproc.p) Besides, we provide items by domain for future work.\nPaper  Abstractive Opinion Tagging\nQintong Li, Piji Li, Xinyi Li, Zhaochun Ren, Zhumin Chen, and Maarten de Rijke.\n@inproceedings{li2020aot, title={Abstractive Opinion Tagging}, author={Qintong, Li and Piji, Li and Xinyi Li and Zhaochun, Ren and Zhumin, Chen and Maarten, de Rijke}, booktitle={WSDM}, year={2021} }  Contact Please reach out to qtleo@outlook.com for any questions about the dataset.\n","date":1613088000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613088000,"objectID":"8da99d8c7a684e342ae775bf9a75ea37","permalink":"https://qtli.github.io/project/ecomtag/","publishdate":"2021-02-12T00:00:00Z","relpermalink":"/project/ecomtag/","section":"project","summary":"Our collected eComtag dataset for abstractive opinion tagging research.","tags":["Deep Learning"],"title":"eComTag Dataset","type":"project"},{"authors":["Qintong Li","Piji Li","Xinyi Li","Zhaochun Ren","Zhumin Chen","Maarten de Rijke"],"categories":[],"content":"","date":1602859393,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602859393,"objectID":"de41299e57a113220c897a69a8336ea1","permalink":"https://qtli.github.io/publication/aot/","publishdate":"2020-10-16T22:43:13+08:00","relpermalink":"/publication/aot/","section":"publication","summary":"We propose a new task, abstractive opinion tagging, and a dataset, eComTag, to generate opinion tags based on large volumes of item reviews. (WSDM 2021, oral paper)","tags":["opinion summarization"],"title":"Abstractive Opinion Tagging","type":"publication"},{"authors":[],"categories":[],"content":"A humanized dialogue system is expected to generate empathetic replies, which should be sensitive to the users’ expressed emotion. The task of empathetic dialogue generation is proposed to address this problem. The essential challenges lie in (1) accurately capturing the nuances of human emotion, (2) modelling complex emotional dependencies between conversation partners, and (3) considering the potential of user feedback, which are overlooked by the majority of existing work. In response to this problem, we propose two empathetic dialogue generation frameworks in two different directions.\n  Figure 1: An empathetic dialogue example from dataset EmpatheticDialogues. The emotional-related words are highlighted in blue. \u0026ldquo;Proud\u0026rdquo; is the coarse-grained emotional label. These emotional words are labelled by an external emotion lexicon (Mohammad and Turney, 2013).   As shown in Figure 1, we list an example from the benchmark dataset EmpatheticDialogues (Rashkin et al., 2019). Notably, emotional words in the three serial utterances between interlocutors have nuanced emotional connections, i.e., “new, job” in utterance 1, “amazing, excited” in utterance 2, and “excited” in utterance 3. Without considering fine-grained emotional words, the responses generated by existing methods are trivialand uninformed, even though they expressed the appropriate emotions. Therefore, explicitly modellingthe fine-grained emotional factor is necessary.\nWe propose a multi-resolution adversarial model – EmpDG, to generate more empathetic responses, which is accepted by COLING 2020.\nEmpDG exploits both the coarse-grained dialogue-level and fine-grained token-level emotions, the latter of which helps to better capture the nuances of user emotion. In addition, we introduce an interactive adversarial learning framework which exploits the user feedback, to identify whether the generated responses evoke emotion perceptivity in dialogues. Experimental results show that the proposed approach significantly outperforms the state-of-the-art baselines in both content quality and emotion perceptivity.\n  Figure 2: An example of empathetic dialogues with external knowledge from EmpatheticDialogues. Emotional-related words in the dialogue are highlighted in red color, whereas emotion-related concepts are marked in blue. Numbers in parentheses denote emotional intensity values.   Lacking external knowledge makes it difficult to perceive implicit emotionsfrom limited dialogue history. To address the above challenges,we propose to leverage multi-type knowledge, i.e, the commonsense knowledge and emotional lexicon, to explicitly understand and express emotions in empathetic dialogue generation.\nWe propose a multi-type knowledge aware empathetic dialogue generation framework, to explicitly understand and express emotions in empathetic dialogue generation, which is submitted to AAAI 2021.\nWe first enrich the dialogue history by jointly interactingwith two-type knowledge and construct an emotional contextgraph. Then we introduce a multi-type knowledge-aware context encoder to learn emotional context representations anddistill emotional signals, which are the prerequisites to predicate emotions expressed in responses. Finally, we proposean emotional cross-attention mechanism to exploit the emotional dependencies between the emotional context graph andthe target empathetic response. The proposed framework makes it easier to accurately perceive and appropriately express implicit emotions.\n","date":1602141522,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602141522,"objectID":"8b5bf5a8e2975ce4d7c20dd3c23761a0","permalink":"https://qtli.github.io/project/emp_dia_gen/","publishdate":"2020-10-08T15:18:42+08:00","relpermalink":"/project/emp_dia_gen/","section":"project","summary":"A humanized dialogue system is expected to generate empathetic replies, which should be sensitive to the users’ expressed emotion. The task of empathetic dialogue generation is proposed to address this problem.","tags":[],"title":"Empathetic Dialogue Generation","type":"project"},{"authors":["Qintong Li","Hongshen Chen","Zhaochun Ren","Pengjie Ren","Zhaopeng Tu","Zhumin Chen"],"categories":[],"content":"","date":1601642435,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601642435,"objectID":"5c53a2582314c00b07153a862f3d835b","permalink":"https://qtli.github.io/publication/empdg/","publishdate":"2020-10-02T20:40:35+08:00","relpermalink":"/publication/empdg/","section":"publication","summary":"A multi-resolution adversarial model -- EmpDG, to generate more empathetic responses. (COLING 2020, oral paper)","tags":["empathetic dialogue generation","adversarial learning"],"title":"EmpDG: Multi-resolution Interactive Empathetic Dialogue Generation","type":"publication"},{"authors":[],"categories":[],"content":"Given single-documents or multi-documents, summarizing the opinions expressed of the input is a vital task in NLP. I divide the current researches into two categories: keyphrase generation and opinion summarization.\nKeyphrase generation: A lot of research has been conducted on generating keyphrases tosummarize various types of text. Early approaches to keyphrase generation extract important phrasesfrom the document as the results. Sequence tagging models havebeen applied to identify keyphrases. Retrieval-based approaches utilize a two-step pipeline to extract and rank candidate keyphrases. Sun et al.[1] adopt an extractive graph-based approach, which applies a point network to generate a set of diverse keyphrases. More recently, abstractive approaches havebeen explored. Chan et al.[2] propose a reinforcement learning approach for neural keyphrase generation that encourages a model to generate both sufficient and accurate keyphrases. Wang et al.[3] propose a topic-aware neural keyphrase generation method toidentify topic words. The methods listed above only consider keyphrase generation given a single document. Our work Abstractive Opinion Tagging considers opinion tagging from multiple documents, that is, from all of the reviewsfor a given item.\nOpinion summarization: Opinion summarization has become an emerging research topicin recent years. Early studies on opinion summarization focus onextracting salient sentences from text: Hu and Liu[4] identify item features mentioned in the reviews and thenextract opinion sentences for the identified features. Unsupervised learning methods are utilized to extract a review summary by exploiting review helpfulness ratings[5]. To reduce redundancy, a greedy algorithm is also applied to form the final summaries[6]. Reflecting the most representative opinions from reviewers, manyrecent studies have shown that abstractive approaches are more appropriate for summarizing review text: Geraniet al.[7] utilize a template filling strategy to generate a review summary; Wang and Ling[8] apply an encoder-decoder attentionmodel to generate an abstractive summary for opinionated documents. The objective of the above summarization approaches is togenerate coherent sentences to summarize opinions.\nIn contrast, we propose the abstractive opinion tagging task so as to generate opinion tags from a large number of user-generated reviews. In our scenario, opinion tags are more concise but without loss of essential information; they should help users comprehend reviews quickly and conveniently (this work has been accepted by WSDM 2021).\nReferences\n[1] Zhiqing Sun, Jian Tang, Pan Du, Zhi-Hong Deng, and Jian-Yun Nie. 2019. Div-GraphPointer: A Graph Pointer Network for Extracting Diverse Keyphrases. In SIGIR.\n[2] Hou Pong Chan, Wang Chen, Lu Wang, and Irwin King. 2019. Neural KeyphraseGeneration via Reinforcement Learning with Adaptive Rewards. In ACL.\n[3] Yue Wang, Jing Li, Hou Pong Chan, Irwin King, Michael R. Lyu, and Shuming Shi. 2019. Topic-Aware Neural Keyphrase Generation for Social Media Language. In ACL.\n[4] Minqing Hu and Bing Liu. 2004. Mining and Summarizing Customer Reviews. In SIGKDD.\n[5] Wenting Xiong and Diane J. Litman. 2014. Empirical Analysis of Exploiting Review Helpfulness for Extractive Summarization of Online Reviews. In COLING.\n[6] Stefanos Angelidis and Mirella Lapata. 2018. Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised. In EMNLP.\n[7] Shima Gerani, Yashar Mehdad, Giuseppe Carenini, Raymond T. Ng, and BitaNejat. 2014. Abstractive Summarization of Product Reviews Using Discourse Structure. In EMNLP.\n[8] Lu Wang and Wang Ling. 2016. Neural Network-Based Abstract Generation for Opinions and Arguments. In NAACL.\n","date":1597565539,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597565539,"objectID":"d26099eeb30179a130a5ccacc121470d","permalink":"https://qtli.github.io/project/summarization/","publishdate":"2020-08-16T16:12:19+08:00","relpermalink":"/project/summarization/","section":"project","summary":"Given single-documents or multi-documents, summarizing the opinions expressed of the input is a vital task in NLP. I divide the current researches into two categories: keyphrase generation and opinion summarization.","tags":[],"title":"Text Summarization","type":"project"}]